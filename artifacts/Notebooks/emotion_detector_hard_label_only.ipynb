{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b115f0-fd0b-485e-89ff-54e12bbd82ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887cabcf-28e0-4108-a9ba-59e74f90914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TORCH_HOME'] = 'D:/torch_cache'  # For pretrained models\n",
    "os.environ['HF_HOME'] = 'D:/huggingface_cache'  # If using Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd34f60-0b6e-4890-b55d-5126ae61c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Use the variables\n",
    "data_dir = os.getenv(\"DATA_DIR\")\n",
    "ground_truth_path = os.getenv(\"GROUND_TRUTH_PATH\")\n",
    "checkpoint_path = os.getenv(\"CHECKPOINT_BEST_PATH\")\n",
    "checkpoint_dir = os.getenv(\"CHECKPOINT_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e66d2d-5860-4b92-9f4c-e71e64bbeae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch modules\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Subset, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0edc198-e139-4c91-9c8d-a3349376c975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ff464-0fb0-47f8-a03a-2184d0203de5",
   "metadata": {},
   "source": [
    "#### Data loading and augmentation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1edc416-fdf0-4a6b-a15a-9cbd3056cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    # Spatial augmentations (scenes tolerate more variation than cars)\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),  # Slightly more rotation allowed\n",
    "    \n",
    "    # Color augmentations (emotions are sensitive to color/tone)\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "    \n",
    "    # Additional augmentations for small dataset\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Small shifts\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Conforms to ImageNet normalization \n",
    "    \n",
    "    # Regularization (critical for ~2K images)\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.15))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff061afe-da54-4042-b56a-06253f264340",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize shorter side to 256\n",
    "    transforms.CenterCrop(224),  # Standard crop\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "356fd532-f306-4296-8c90-2ad41d587ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset WITHOUT transforms (for indexing only)\n",
    "data_dir = r\"training/dataset/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eac812a-496c-4039-b822-46a0d2ea21b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1980\n",
      "Class names: ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load dataset without transform to get targets for stratification\n",
    "full_dataset = ImageFolder(data_dir, transform=None)\n",
    "targets = np.array(full_dataset.targets)\n",
    "\n",
    "print(f\"Total samples: {len(full_dataset)}\")\n",
    "print(f\"Class names: {full_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2f4ec9-0198-4c5b-b684-1dd8998aa2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: First split - 70% train, 30% temp (val + test)\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    np.arange(len(targets)),\n",
    "    test_size=0.30,           # 30% for val + test combined\n",
    "    stratify=targets,         # Ensures equal class proportions\n",
    "    random_state=42           # For reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a28fecb-9bae-4f62-b9ff-e9df30b21db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Second split - divide temp into 15% val and 15% test\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.5,            # 50% of 30% = 15% of total\n",
    "    stratify=targets[temp_idx],  # Maintain stratification\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ded31027-e78b-4439-a327-be89cd6078e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Subsets\n",
    "train_subset = Subset(full_dataset, train_idx)\n",
    "val_subset = Subset(full_dataset, val_idx)\n",
    "test_subset = Subset(full_dataset, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a1179ae-1b06-467e-9b05-29f7957fc870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Wrapper to apply different transforms to different subsets\n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get PIL Image and label from subset\n",
    "        image, label = self.subset[idx]\n",
    "        \n",
    "        # Apply transform if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ce94da8-98ab-44ce-8030-87be0e5f9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transforms\n",
    "train_dataset = TransformDataset(train_subset, train_transform)\n",
    "val_dataset = TransformDataset(val_subset, eval_transform)\n",
    "test_dataset = TransformDataset(test_subset, eval_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a7227cc-1504-4e54-92fc-b9457d870f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create DataLoaders\n",
    "batch_size = 32\n",
    "num_workers = 0  # Use 0 on Windows to avoid multiprocessing issues; use 4 on Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "491fb836-a076-4c5c-8cf2-e2f87714e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,           # Only training set gets shuffled\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,          # No shuffle for validation\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,          # No shuffle for test\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9efc40dc-c433-44fc-bc06-bfb97a367675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Check class distribution\n",
    "def check_class_distribution(subset, targets, name):\n",
    "    subset_targets = targets[subset.indices]\n",
    "    unique, counts = np.unique(subset_targets, return_counts=True)\n",
    "    print(f\"\\n{name} set ({len(subset)} samples):\")\n",
    "    for cls_idx, count in zip(unique, counts):\n",
    "        print(f\"  {full_dataset.classes[cls_idx]}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a478127d-08d8-401c-bddd-0e67dfba1431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set (1386 samples):\n",
      "  anger: 231\n",
      "  disgust: 231\n",
      "  fear: 231\n",
      "  joy: 231\n",
      "  sadness: 231\n",
      "  surprise: 231\n",
      "\n",
      "Validation set (297 samples):\n",
      "  anger: 50\n",
      "  disgust: 50\n",
      "  fear: 49\n",
      "  joy: 49\n",
      "  sadness: 49\n",
      "  surprise: 50\n",
      "\n",
      "Test set (297 samples):\n",
      "  anger: 49\n",
      "  disgust: 49\n",
      "  fear: 50\n",
      "  joy: 50\n",
      "  sadness: 50\n",
      "  surprise: 49\n"
     ]
    }
   ],
   "source": [
    "check_class_distribution(train_subset, targets, \"Train\")\n",
    "check_class_distribution(val_subset, targets, \"Validation\")\n",
    "check_class_distribution(test_subset, targets, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e864b336-9058-45dc-bdb7-176fae1e8d45",
   "metadata": {},
   "source": [
    "#### Training & validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "208dab5f-c5d5-4d64-a0be-5451435935d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epochs,\n",
    "    num_classes=6,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    class_names=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains and validates the model for multi-class classification.\n",
    "    Saves checkpoint after each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    assert epochs % 1 == 0, \"Epochs must be in steps of 1\"\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    best_val_acc = 0.0  # Track best model\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # --------------------\n",
    "        # Training phase\n",
    "        # --------------------\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # --------------------\n",
    "        # Validation phase\n",
    "        # --------------------\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        all_true_labels = []\n",
    "        all_pred_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                all_true_labels.extend(labels.cpu().numpy())\n",
    "                all_pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        accuracy = 100.0 * correct / total\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        # --------------------\n",
    "        # Logging\n",
    "        # --------------------\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "            f\"Val Acc: {accuracy:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # --------------------\n",
    "        # Save checkpoint (every epoch)\n",
    "        # --------------------\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_acc': accuracy,\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # --------------------\n",
    "        # Save best model separately\n",
    "        # --------------------\n",
    "        if accuracy > best_val_acc:\n",
    "            best_val_acc = accuracy\n",
    "            best_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  **** New best model saved! (Acc: {accuracy:.2f}%)\")\n",
    "\n",
    "    # --------------------\n",
    "    # Final evaluation metrics (after all epochs)\n",
    "    # --------------------\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training completed! Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Classification report for multi-class\n",
    "    if class_names:\n",
    "        print(\"\\nClassification Report (Last Epoch):\")\n",
    "        print(classification_report(all_true_labels, all_pred_labels, target_names=class_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_true_labels, all_pred_labels)\n",
    "    print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies, all_true_labels, all_pred_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2f41a2f-b955-4055-b8fd-3cda2ba0506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained weights with explicit cache location\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Freeze all parameters first\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace fully connected layer with 6 outputs\n",
    "# --------------------------------------------------\n",
    "# Get input features of original fc layer (typically 2048 for ResNet50)\n",
    "in_features = model.fc.in_features\n",
    "\n",
    "# Replace with new fc layer (unfrozen by default)\n",
    "model.fc = nn.Linear(in_features, 6)\n",
    "\n",
    "# Move ENTIRE model to device AFTER architecture changes\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69674fd9-ae1f-440f-a351-9a80d410fc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "  fc.weight: torch.Size([6, 2048])\n",
      "  fc.bias: torch.Size([6])\n",
      "\n",
      "Total parameters: 23,520,326\n",
      "Trainable parameters: 12,294\n",
      "Frozen parameters: 23,508,032\n",
      "Percentage trainable: 0.05%\n"
     ]
    }
   ],
   "source": [
    "# Verification: Check which parameters are trainable\n",
    "\n",
    "print(\"Trainable parameters:\")\n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33d5e867-ec52-4d17-8efb-b6fb316a44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Class names\n",
    "emotion_classes = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54957e85-7e10-458a-8e6f-8db0e4ca44aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] | Train Loss: 1.5672 | Val Loss: 1.3876 | Val Acc: 46.80%\n",
      "  **** New best model saved! (Acc: 46.80%)\n",
      "Epoch [2/30] | Train Loss: 1.2862 | Val Loss: 1.2897 | Val Acc: 49.83%\n",
      "  **** New best model saved! (Acc: 49.83%)\n",
      "Epoch [3/30] | Train Loss: 1.1664 | Val Loss: 1.2311 | Val Acc: 51.52%\n",
      "  **** New best model saved! (Acc: 51.52%)\n",
      "Epoch [4/30] | Train Loss: 1.1209 | Val Loss: 1.2050 | Val Acc: 53.20%\n",
      "  **** New best model saved! (Acc: 53.20%)\n",
      "Epoch [5/30] | Train Loss: 1.0769 | Val Loss: 1.1806 | Val Acc: 52.53%\n",
      "Epoch [6/30] | Train Loss: 1.0442 | Val Loss: 1.1653 | Val Acc: 54.55%\n",
      "  **** New best model saved! (Acc: 54.55%)\n",
      "Epoch [7/30] | Train Loss: 0.9954 | Val Loss: 1.1603 | Val Acc: 51.85%\n",
      "Epoch [8/30] | Train Loss: 0.9567 | Val Loss: 1.1680 | Val Acc: 53.20%\n",
      "Epoch [9/30] | Train Loss: 0.9150 | Val Loss: 1.1539 | Val Acc: 54.21%\n",
      "Epoch [10/30] | Train Loss: 0.9035 | Val Loss: 1.1553 | Val Acc: 54.21%\n",
      "Epoch [11/30] | Train Loss: 0.8866 | Val Loss: 1.1429 | Val Acc: 53.20%\n",
      "Epoch [12/30] | Train Loss: 0.8629 | Val Loss: 1.1498 | Val Acc: 53.54%\n",
      "Epoch [13/30] | Train Loss: 0.8417 | Val Loss: 1.1632 | Val Acc: 53.20%\n",
      "Epoch [14/30] | Train Loss: 0.8492 | Val Loss: 1.1489 | Val Acc: 52.86%\n",
      "Epoch [15/30] | Train Loss: 0.8285 | Val Loss: 1.1524 | Val Acc: 54.88%\n",
      "  **** New best model saved! (Acc: 54.88%)\n",
      "Epoch [16/30] | Train Loss: 0.8031 | Val Loss: 1.1539 | Val Acc: 56.90%\n",
      "  **** New best model saved! (Acc: 56.90%)\n",
      "Epoch [17/30] | Train Loss: 0.8011 | Val Loss: 1.1551 | Val Acc: 52.86%\n",
      "Epoch [18/30] | Train Loss: 0.7728 | Val Loss: 1.1539 | Val Acc: 54.88%\n",
      "Epoch [19/30] | Train Loss: 0.7832 | Val Loss: 1.1547 | Val Acc: 57.58%\n",
      "  **** New best model saved! (Acc: 57.58%)\n",
      "Epoch [20/30] | Train Loss: 0.7619 | Val Loss: 1.1685 | Val Acc: 53.54%\n",
      "Epoch [21/30] | Train Loss: 0.7513 | Val Loss: 1.1621 | Val Acc: 52.86%\n",
      "Epoch [22/30] | Train Loss: 0.7187 | Val Loss: 1.1487 | Val Acc: 54.55%\n",
      "Epoch [23/30] | Train Loss: 0.7232 | Val Loss: 1.1681 | Val Acc: 54.21%\n",
      "Epoch [24/30] | Train Loss: 0.7100 | Val Loss: 1.1711 | Val Acc: 53.87%\n",
      "Epoch [25/30] | Train Loss: 0.7038 | Val Loss: 1.1666 | Val Acc: 54.55%\n",
      "Epoch [26/30] | Train Loss: 0.7062 | Val Loss: 1.1810 | Val Acc: 53.20%\n",
      "Epoch [27/30] | Train Loss: 0.6933 | Val Loss: 1.1832 | Val Acc: 54.21%\n",
      "Epoch [28/30] | Train Loss: 0.6718 | Val Loss: 1.1870 | Val Acc: 53.54%\n",
      "Epoch [29/30] | Train Loss: 0.6533 | Val Loss: 1.1986 | Val Acc: 54.55%\n",
      "Epoch [30/30] | Train Loss: 0.6731 | Val Loss: 1.1618 | Val Acc: 55.22%\n",
      "\n",
      "==================================================\n",
      "Training completed! Best validation accuracy: 57.58%\n",
      "==================================================\n",
      "\n",
      "Classification Report (Last Epoch):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.54      0.44      0.48        50\n",
      "     disgust       0.75      0.86      0.80        50\n",
      "        fear       0.40      0.37      0.38        49\n",
      "         joy       0.53      0.63      0.58        49\n",
      "     sadness       0.53      0.53      0.53        49\n",
      "    surprise       0.51      0.48      0.49        50\n",
      "\n",
      "    accuracy                           0.55       297\n",
      "   macro avg       0.54      0.55      0.55       297\n",
      "weighted avg       0.55      0.55      0.55       297\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[22  2 10  2  6  8]\n",
      " [ 0 43  3  1  1  2]\n",
      " [ 7  7 18  4 11  2]\n",
      " [ 3  1  6 31  2  6]\n",
      " [ 3  2  6  7 26  5]\n",
      " [ 6  2  2 13  3 24]]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train_losses, val_losses, val_accs, true_labels, pred_labels = train_and_validate(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    epochs=30,\n",
    "    num_classes=6,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    class_names=emotion_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "276bb567-7fd7-417e-b558-acf360dd888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Train only FC (already done - 30 epochs, 57.58%)\n",
    "\n",
    "# Phase 2: Unfreeze layer4 + train FC (30 epochs)\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Use differential learning rates: lower for pretrained layers\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.layer4.parameters(), 'lr': 1e-5}, # Deeper layers: slower\n",
    "    {'params': model.fc.parameters(), 'lr': 1e-4} # New layer: faster\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f15d006e-318d-488c-8aae-80b76d8b9b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] | Train Loss: 0.6409 | Val Loss: 1.1670 | Val Acc: 54.55%\n",
      "  **** New best model saved! (Acc: 54.55%)\n",
      "Epoch [2/30] | Train Loss: 0.6391 | Val Loss: 1.1732 | Val Acc: 56.23%\n",
      "  **** New best model saved! (Acc: 56.23%)\n",
      "Epoch [3/30] | Train Loss: 0.6023 | Val Loss: 1.1780 | Val Acc: 55.22%\n",
      "Epoch [4/30] | Train Loss: 0.6006 | Val Loss: 1.1667 | Val Acc: 57.58%\n",
      "  **** New best model saved! (Acc: 57.58%)\n",
      "Epoch [5/30] | Train Loss: 0.5658 | Val Loss: 1.1795 | Val Acc: 57.24%\n",
      "Epoch [6/30] | Train Loss: 0.5553 | Val Loss: 1.2019 | Val Acc: 56.57%\n",
      "Epoch [7/30] | Train Loss: 0.5034 | Val Loss: 1.1931 | Val Acc: 56.90%\n",
      "Epoch [8/30] | Train Loss: 0.5308 | Val Loss: 1.2111 | Val Acc: 57.58%\n",
      "Epoch [9/30] | Train Loss: 0.5106 | Val Loss: 1.2109 | Val Acc: 57.58%\n",
      "Epoch [10/30] | Train Loss: 0.4761 | Val Loss: 1.2215 | Val Acc: 57.24%\n",
      "Epoch [11/30] | Train Loss: 0.4676 | Val Loss: 1.2181 | Val Acc: 58.59%\n",
      "  **** New best model saved! (Acc: 58.59%)\n",
      "Epoch [12/30] | Train Loss: 0.4549 | Val Loss: 1.2236 | Val Acc: 57.91%\n",
      "Epoch [13/30] | Train Loss: 0.4175 | Val Loss: 1.2143 | Val Acc: 56.57%\n",
      "Epoch [14/30] | Train Loss: 0.4244 | Val Loss: 1.2423 | Val Acc: 57.58%\n",
      "Epoch [15/30] | Train Loss: 0.4170 | Val Loss: 1.2314 | Val Acc: 58.25%\n",
      "Epoch [16/30] | Train Loss: 0.4066 | Val Loss: 1.2479 | Val Acc: 57.91%\n",
      "Epoch [17/30] | Train Loss: 0.3872 | Val Loss: 1.2563 | Val Acc: 58.25%\n",
      "Epoch [18/30] | Train Loss: 0.3685 | Val Loss: 1.2653 | Val Acc: 57.58%\n",
      "Epoch [19/30] | Train Loss: 0.3701 | Val Loss: 1.2252 | Val Acc: 57.91%\n",
      "Epoch [20/30] | Train Loss: 0.3595 | Val Loss: 1.2601 | Val Acc: 59.60%\n",
      "  **** New best model saved! (Acc: 59.60%)\n",
      "Epoch [21/30] | Train Loss: 0.3356 | Val Loss: 1.2497 | Val Acc: 57.58%\n",
      "Epoch [22/30] | Train Loss: 0.3124 | Val Loss: 1.2330 | Val Acc: 56.23%\n",
      "Epoch [23/30] | Train Loss: 0.3525 | Val Loss: 1.2822 | Val Acc: 57.91%\n",
      "Epoch [24/30] | Train Loss: 0.3176 | Val Loss: 1.2955 | Val Acc: 58.59%\n",
      "Epoch [25/30] | Train Loss: 0.3008 | Val Loss: 1.2981 | Val Acc: 57.58%\n",
      "Epoch [26/30] | Train Loss: 0.2978 | Val Loss: 1.2848 | Val Acc: 59.26%\n",
      "Epoch [27/30] | Train Loss: 0.2893 | Val Loss: 1.3051 | Val Acc: 59.26%\n",
      "Epoch [28/30] | Train Loss: 0.2767 | Val Loss: 1.3126 | Val Acc: 58.92%\n",
      "Epoch [29/30] | Train Loss: 0.2669 | Val Loss: 1.3258 | Val Acc: 58.59%\n",
      "Epoch [30/30] | Train Loss: 0.2624 | Val Loss: 1.3025 | Val Acc: 59.93%\n",
      "  **** New best model saved! (Acc: 59.93%)\n",
      "\n",
      "==================================================\n",
      "Training completed! Best validation accuracy: 59.93%\n",
      "==================================================\n",
      "\n",
      "Classification Report (Last Epoch):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.57      0.56      0.57        50\n",
      "     disgust       0.83      0.86      0.84        50\n",
      "        fear       0.47      0.41      0.43        49\n",
      "         joy       0.61      0.67      0.64        49\n",
      "     sadness       0.55      0.59      0.57        49\n",
      "    surprise       0.54      0.50      0.52        50\n",
      "\n",
      "    accuracy                           0.60       297\n",
      "   macro avg       0.59      0.60      0.60       297\n",
      "weighted avg       0.59      0.60      0.60       297\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[28  0  7  2  7  6]\n",
      " [ 1 43  2  1  1  2]\n",
      " [ 7  5 20  1 13  3]\n",
      " [ 4  1  5 33  1  5]\n",
      " [ 3  0  7  5 29  5]\n",
      " [ 6  3  2 12  2 25]]\n"
     ]
    }
   ],
   "source": [
    "# 3. Continue training 30 epochs\n",
    "train_losses, val_losses, val_accs, true_labels, pred_labels = train_and_validate(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    epochs=30,  \n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    class_names=emotion_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e5bf2-d3a1-435b-9f9f-95295d96a7f8",
   "metadata": {},
   "source": [
    "#### Modified training loop with LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fabebb7f-c316-4f0f-9c96-091e15ee9bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_LR(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epochs,\n",
    "    scheduler=None,           # NEW: Add scheduler parameter\n",
    "    num_classes=6,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    class_names=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains and validates the model with scheduler support.\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # --------------------\n",
    "        # Training phase (same as before)\n",
    "        # --------------------\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # --------------------\n",
    "        # Validation phase (same as before)\n",
    "        # --------------------\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        all_true_labels = []\n",
    "        all_pred_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                all_true_labels.extend(labels.cpu().numpy())\n",
    "                all_pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        accuracy = 100.0 * correct / total\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        # --------------------\n",
    "        # Logging (same)\n",
    "        # --------------------\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "              f\"Val Acc: {accuracy:.2f}%\")\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # NEW: Step the scheduler based on validation accuracy\n",
    "        # --------------------------------------------------\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(accuracy)  # Pass current validation accuracy\n",
    "\n",
    "        # --------------------\n",
    "        # Save checkpoint (same)\n",
    "        # --------------------\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_acc': accuracy,\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # Save best model\n",
    "        if accuracy > best_val_acc:\n",
    "            best_val_acc = accuracy\n",
    "            best_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  **** New best model saved! (Acc: {accuracy:.2f}%)\")\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training completed! Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies, all_true_labels, all_pred_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb8ad4-f52e-4460-9843-3e38d326f1a9",
   "metadata": {},
   "source": [
    "#### Use regularization & dropout in FC layer, unfreeze convolutions layers 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91c565b3-b25d-4527-88e5-43ee73f3020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameter groups:\n",
      "  layer3.0.conv1.weight: torch.Size([256, 512, 1, 1])\n",
      "  layer3.0.bn1.weight: torch.Size([256])\n",
      "  layer3.0.bn1.bias: torch.Size([256])\n",
      "  layer3.0.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "  layer3.0.bn2.weight: torch.Size([256])\n",
      "  layer3.0.bn2.bias: torch.Size([256])\n",
      "  layer3.0.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "  layer3.0.bn3.weight: torch.Size([1024])\n",
      "  layer3.0.bn3.bias: torch.Size([1024])\n",
      "  layer3.0.downsample.0.weight: torch.Size([1024, 512, 1, 1])\n",
      "  layer3.0.downsample.1.weight: torch.Size([1024])\n",
      "  layer3.0.downsample.1.bias: torch.Size([1024])\n",
      "  layer3.1.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "  layer3.1.bn1.weight: torch.Size([256])\n",
      "  layer3.1.bn1.bias: torch.Size([256])\n",
      "  layer3.1.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "  layer3.1.bn2.weight: torch.Size([256])\n",
      "  layer3.1.bn2.bias: torch.Size([256])\n",
      "  layer3.1.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "  layer3.1.bn3.weight: torch.Size([1024])\n",
      "  layer3.1.bn3.bias: torch.Size([1024])\n",
      "  layer3.2.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "  layer3.2.bn1.weight: torch.Size([256])\n",
      "  layer3.2.bn1.bias: torch.Size([256])\n",
      "  layer3.2.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "  layer3.2.bn2.weight: torch.Size([256])\n",
      "  layer3.2.bn2.bias: torch.Size([256])\n",
      "  layer3.2.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "  layer3.2.bn3.weight: torch.Size([1024])\n",
      "  layer3.2.bn3.bias: torch.Size([1024])\n",
      "  layer3.3.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "  layer3.3.bn1.weight: torch.Size([256])\n",
      "  layer3.3.bn1.bias: torch.Size([256])\n",
      "  layer3.3.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "  layer3.3.bn2.weight: torch.Size([256])\n",
      "  layer3.3.bn2.bias: torch.Size([256])\n",
      "  layer3.3.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "  layer3.3.bn3.weight: torch.Size([1024])\n",
      "  layer3.3.bn3.bias: torch.Size([1024])\n",
      "  layer3.4.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "  layer3.4.bn1.weight: torch.Size([256])\n",
      "  layer3.4.bn1.bias: torch.Size([256])\n",
      "  layer3.4.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "  layer3.4.bn2.weight: torch.Size([256])\n",
      "  layer3.4.bn2.bias: torch.Size([256])\n",
      "  layer3.4.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "  layer3.4.bn3.weight: torch.Size([1024])\n",
      "  layer3.4.bn3.bias: torch.Size([1024])\n",
      "  layer3.5.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "  layer3.5.bn1.weight: torch.Size([256])\n",
      "  layer3.5.bn1.bias: torch.Size([256])\n",
      "  layer3.5.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "  layer3.5.bn2.weight: torch.Size([256])\n",
      "  layer3.5.bn2.bias: torch.Size([256])\n",
      "  layer3.5.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "  layer3.5.bn3.weight: torch.Size([1024])\n",
      "  layer3.5.bn3.bias: torch.Size([1024])\n",
      "  layer4.0.conv1.weight: torch.Size([512, 1024, 1, 1])\n",
      "  layer4.0.bn1.weight: torch.Size([512])\n",
      "  layer4.0.bn1.bias: torch.Size([512])\n",
      "  layer4.0.conv2.weight: torch.Size([512, 512, 3, 3])\n",
      "  layer4.0.bn2.weight: torch.Size([512])\n",
      "  layer4.0.bn2.bias: torch.Size([512])\n",
      "  layer4.0.conv3.weight: torch.Size([2048, 512, 1, 1])\n",
      "  layer4.0.bn3.weight: torch.Size([2048])\n",
      "  layer4.0.bn3.bias: torch.Size([2048])\n",
      "  layer4.0.downsample.0.weight: torch.Size([2048, 1024, 1, 1])\n",
      "  layer4.0.downsample.1.weight: torch.Size([2048])\n",
      "  layer4.0.downsample.1.bias: torch.Size([2048])\n",
      "  layer4.1.conv1.weight: torch.Size([512, 2048, 1, 1])\n",
      "  layer4.1.bn1.weight: torch.Size([512])\n",
      "  layer4.1.bn1.bias: torch.Size([512])\n",
      "  layer4.1.conv2.weight: torch.Size([512, 512, 3, 3])\n",
      "  layer4.1.bn2.weight: torch.Size([512])\n",
      "  layer4.1.bn2.bias: torch.Size([512])\n",
      "  layer4.1.conv3.weight: torch.Size([2048, 512, 1, 1])\n",
      "  layer4.1.bn3.weight: torch.Size([2048])\n",
      "  layer4.1.bn3.bias: torch.Size([2048])\n",
      "  layer4.2.conv1.weight: torch.Size([512, 2048, 1, 1])\n",
      "  layer4.2.bn1.weight: torch.Size([512])\n",
      "  layer4.2.bn1.bias: torch.Size([512])\n",
      "  layer4.2.conv2.weight: torch.Size([512, 512, 3, 3])\n",
      "  layer4.2.bn2.weight: torch.Size([512])\n",
      "  layer4.2.bn2.bias: torch.Size([512])\n",
      "  layer4.2.conv3.weight: torch.Size([2048, 512, 1, 1])\n",
      "  layer4.2.bn3.weight: torch.Size([2048])\n",
      "  layer4.2.bn3.bias: torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ResNet50\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 1: Replace FC with improved architecture (dropout + deeper layers)\n",
    "# --------------------------------------------------\n",
    "in_features = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),              # Prevent overfitting\n",
    "    nn.Linear(in_features, 512),  # Intermediate layer\n",
    "    nn.ReLU(),                    # Non-linearity\n",
    "    nn.Dropout(0.3),              # Additional regularization\n",
    "    nn.Linear(512, 6)             # 6 emotion classes\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 2: Unfreeze layer3 AND layer4 for fine-tuning\n",
    "# --------------------------------------------------\n",
    "# First freeze everything\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze layer3 and layer4 (high-level features)\n",
    "for param in model.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# FC layer is already trainable (new parameters have requires_grad=True by default)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Step 3: Move to device AFTER all modifications\n",
    "# --------------------------------------------------\n",
    "model = model.to(device)\n",
    "\n",
    "# Verify trainable parameters\n",
    "print(\"Trainable parameter groups:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"  {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45acdcc7-4044-46fb-9089-2b0c9539a467",
   "metadata": {},
   "source": [
    "#### Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abd526ab-b8e1-4dd6-b58f-dad7b46a6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Differential learning rates\n",
    "# --------------------------------------------------\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.layer3.parameters(), 'lr': 1e-6},   # Deepest conv: slowest\n",
    "    {'params': model.layer4.parameters(), 'lr': 1e-5},   # Deep conv: slow\n",
    "    {'params': model.fc.parameters(), 'lr': 1e-4}        # New layers: faster\n",
    "])\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Learning rate scheduler - ReduceLROnPlateau\n",
    "# --------------------------------------------------\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='max',           # Monitor validation accuracy (maximize)\n",
    "    factor=0.5,           # Reduce LR by half when plateau\n",
    "    patience=5,           # Wait 5 epochs before reducing\n",
    "    min_lr=1e-7           # Don't go below this\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e04a725-2825-4aa1-b325-9096454c27f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] | Train Loss: 1.7943 | Val Loss: 1.7869 | Val Acc: 20.88%\n",
      "  **** New best model saved! (Acc: 20.88%)\n",
      "Epoch [2/25] | Train Loss: 1.7918 | Val Loss: 1.7845 | Val Acc: 22.22%\n",
      "  **** New best model saved! (Acc: 22.22%)\n",
      "Epoch [3/25] | Train Loss: 1.7895 | Val Loss: 1.7826 | Val Acc: 22.22%\n",
      "Epoch [4/25] | Train Loss: 1.7835 | Val Loss: 1.7774 | Val Acc: 23.91%\n",
      "  **** New best model saved! (Acc: 23.91%)\n",
      "Epoch [5/25] | Train Loss: 1.7820 | Val Loss: 1.7733 | Val Acc: 26.60%\n",
      "  **** New best model saved! (Acc: 26.60%)\n",
      "Epoch [6/25] | Train Loss: 1.7764 | Val Loss: 1.7693 | Val Acc: 26.94%\n",
      "  **** New best model saved! (Acc: 26.94%)\n",
      "Epoch [7/25] | Train Loss: 1.7737 | Val Loss: 1.7668 | Val Acc: 29.63%\n",
      "  **** New best model saved! (Acc: 29.63%)\n",
      "Epoch [8/25] | Train Loss: 1.7682 | Val Loss: 1.7583 | Val Acc: 31.65%\n",
      "  **** New best model saved! (Acc: 31.65%)\n",
      "Epoch [9/25] | Train Loss: 1.7599 | Val Loss: 1.7591 | Val Acc: 32.66%\n",
      "  **** New best model saved! (Acc: 32.66%)\n",
      "Epoch [10/25] | Train Loss: 1.7500 | Val Loss: 1.7471 | Val Acc: 34.34%\n",
      "  **** New best model saved! (Acc: 34.34%)\n",
      "Epoch [11/25] | Train Loss: 1.7423 | Val Loss: 1.7432 | Val Acc: 35.35%\n",
      "  **** New best model saved! (Acc: 35.35%)\n",
      "Epoch [12/25] | Train Loss: 1.7310 | Val Loss: 1.7279 | Val Acc: 37.04%\n",
      "  **** New best model saved! (Acc: 37.04%)\n",
      "Epoch [13/25] | Train Loss: 1.7204 | Val Loss: 1.7130 | Val Acc: 39.39%\n",
      "  **** New best model saved! (Acc: 39.39%)\n",
      "Epoch [14/25] | Train Loss: 1.7145 | Val Loss: 1.7073 | Val Acc: 41.08%\n",
      "  **** New best model saved! (Acc: 41.08%)\n",
      "Epoch [15/25] | Train Loss: 1.6856 | Val Loss: 1.6739 | Val Acc: 42.09%\n",
      "  **** New best model saved! (Acc: 42.09%)\n",
      "Epoch [16/25] | Train Loss: 1.6634 | Val Loss: 1.6435 | Val Acc: 42.76%\n",
      "  **** New best model saved! (Acc: 42.76%)\n",
      "Epoch [17/25] | Train Loss: 1.6370 | Val Loss: 1.6138 | Val Acc: 44.11%\n",
      "  **** New best model saved! (Acc: 44.11%)\n",
      "Epoch [18/25] | Train Loss: 1.6103 | Val Loss: 1.6058 | Val Acc: 44.78%\n",
      "  **** New best model saved! (Acc: 44.78%)\n",
      "Epoch [19/25] | Train Loss: 1.5754 | Val Loss: 1.5492 | Val Acc: 44.11%\n",
      "Epoch [20/25] | Train Loss: 1.5660 | Val Loss: 1.5102 | Val Acc: 44.44%\n",
      "Epoch [21/25] | Train Loss: 1.5397 | Val Loss: 1.4993 | Val Acc: 44.78%\n",
      "Epoch [22/25] | Train Loss: 1.5170 | Val Loss: 1.5028 | Val Acc: 45.79%\n",
      "  **** New best model saved! (Acc: 45.79%)\n",
      "Epoch [23/25] | Train Loss: 1.4962 | Val Loss: 1.4728 | Val Acc: 43.77%\n",
      "Epoch [24/25] | Train Loss: 1.4691 | Val Loss: 1.4661 | Val Acc: 46.80%\n",
      "  **** New best model saved! (Acc: 46.80%)\n",
      "Epoch [25/25] | Train Loss: 1.4610 | Val Loss: 1.4533 | Val Acc: 46.80%\n",
      "\n",
      "==================================================\n",
      "Training completed! Best validation accuracy: 46.80%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Train for 25 epochs with scheduler\n",
    "train_losses, val_losses, val_accs, true_labels, pred_labels = train_and_validate_LR(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    epochs=25,                          # Increased to 25\n",
    "    scheduler=scheduler,                # Pass the scheduler\n",
    "    num_classes=6,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    class_names=['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a42ac53-db86-4736-866d-2c9f6c84fce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e218aa-e36f-40c4-b9b9-db28f7f65db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00db0dc-29ef-4e9a-9634-cbd03a299570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0fb7dc-3950-4ef6-b014-38a43345f19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d532422-c067-438d-a0b7-a11e5dcdad25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1928d41a-a719-4dd1-bc07-6a7994a33f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f907b3c-1964-461c-bab2-c6d69b26e144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd666914-44b1-4869-bb9a-f2ff5f8f9e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c0e71-bf0b-4cfa-bbdb-a942d0f623b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9413c6-2e1f-4c9b-b70b-7cc54cd353d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (CB)",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
