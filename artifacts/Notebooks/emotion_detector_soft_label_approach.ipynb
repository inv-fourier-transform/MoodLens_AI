{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd11a6a-c1d1-4331-a81e-bc3bed04826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5128dfb-2268-485c-b966-35b540a28db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Use the variables\n",
    "data_dir = os.getenv(\"DATA_DIR\")\n",
    "ground_truth_path = os.getenv(\"GROUND_TRUTH_PATH\")\n",
    "checkpoint_path = os.getenv(\"CHECKPOINT_BEST_PATH\")\n",
    "checkpoint_dir = os.getenv(\"CHECKPOINT_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b54733e-91b9-4eb1-ae6e-b6d581be8813",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TORCH_HOME'] = 'D:/torch_cache'  # For pretrained models\n",
    "os.environ['HF_HOME'] = 'D:/huggingface_cache'  # If using Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a6e8a3-3872-4e62-96a3-ac45e74d17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch modules\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Subset, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b84a2b-dc7d-4d89-b836-323121b5e144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf18440d-ab66-4063-b21e-981a2aec85a2",
   "metadata": {},
   "source": [
    "#### CUSTOM DATASET FOR SOFT LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ed1f25-c00a-4fbc-89fe-e918fcb9ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emotion6SoftDataset(Dataset):\n",
    "    def __init__(self, data_dir, ground_truth_file, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Read ground truth file\n",
    "        df = pd.read_csv(ground_truth_file, sep='\\t')\n",
    "        \n",
    "        # Debug: print actual column names\n",
    "        print(\"Available columns:\", df.columns.tolist())\n",
    "        \n",
    "        # Find the image filename column (handles both 'image_filename' and '[image_filename]')\n",
    "        image_cols = [c for c in df.columns if 'image_filename' in c.lower()]\n",
    "        if len(image_cols) == 0:\n",
    "            raise KeyError(\"No column containing 'image_filename' found\")\n",
    "        self.image_col = image_cols[0]\n",
    "        \n",
    "        # Find probability columns\n",
    "        self.prob_cols = [c for c in df.columns if 'prob.' in c.lower() and 'neutral' not in c.lower()]\n",
    "        self.prob_cols = [c for c in self.prob_cols if any(e in c.lower() for e in ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise'])]\n",
    "        \n",
    "        print(f\"Using image column: {self.image_col}\")\n",
    "        print(f\"Using prob columns: {self.prob_cols}\")\n",
    "        \n",
    "        # Build file paths and soft labels\n",
    "        self.image_paths = []\n",
    "        self.soft_labels = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            img_path = os.path.join(data_dir, row[self.image_col])\n",
    "            soft_label = row[self.prob_cols].values.astype('float32')\n",
    "            \n",
    "            self.image_paths.append(img_path)\n",
    "            self.soft_labels.append(soft_label)\n",
    "        \n",
    "        print(f\"Loaded {len(self)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        soft_label = self.soft_labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(soft_label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe5af1-4054-4685-8fbc-b59204d5d72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "834f7b0f-8433-42c2-b28b-82edf7cc5b91",
   "metadata": {},
   "source": [
    "#### Data loading and augmentation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6716a801-83a6-4684-9184-ae0876da0a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    # Spatial augmentations (scenes tolerate more variation than cars)\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),  # Slightly more rotation allowed\n",
    "    \n",
    "    # Color augmentations (emotions are sensitive to color/tone)\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "    \n",
    "    # Additional augmentations for small dataset\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Small shifts\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Conforms to ImageNet normalization \n",
    "    \n",
    "    # Regularization (critical for ~2K images)\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.15))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37b0b530-44ad-4a9e-b4a1-c9bceae19197",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize shorter side to 256\n",
    "    transforms.CenterCrop(224),  # Standard crop\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010be4bd-b90e-4496-b5c9-1ed645ba82fe",
   "metadata": {},
   "source": [
    "#### CREATE DATASETS WITH STRATIFIED SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ad67533-c6a3-40e4-a861-fbbc768eff9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f3b5cf6-00ac-43cb-9b11-1a7283824b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['[image_filename]', '[valence]', '[arousal]', '[prob. anger]', '[prob. disgust]', '[prob. fear]', '[prob. joy]', '[prob. sadness]', '[prob. surprise]', '[prob. neutral]']\n",
      "Using image column: [image_filename]\n",
      "Using prob columns: ['[prob. anger]', '[prob. disgust]', '[prob. fear]', '[prob. joy]', '[prob. sadness]', '[prob. surprise]']\n",
      "Loaded 1980 samples\n"
     ]
    }
   ],
   "source": [
    "# Create full dataset (without transform initially for stratification)\n",
    "full_dataset = Emotion6SoftDataset(data_dir, ground_truth_path, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "759eabf4-01f3-4638-a27b-d2519f19ef66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1386, Val: 297, Test: 297\n"
     ]
    }
   ],
   "source": [
    "# Get hard labels for stratification (dominant emotion from soft labels)\n",
    "hard_targets = [np.argmax(label) for label in full_dataset.soft_labels]\n",
    "\n",
    "# Stratified split: 70% train, 15% val, 15% test\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    range(len(hard_targets)),\n",
    "    test_size=0.30,\n",
    "    stratify=hard_targets,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.5,\n",
    "    stratify=[hard_targets[i] for i in temp_idx],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a516c60f-dd50-46a9-ada9-17644010f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to apply different transforms\n",
    "class SoftTransformDataset(Dataset):\n",
    "    def __init__(self, base_dataset, indices, transform):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        img_path = self.base_dataset.image_paths[real_idx]\n",
    "        soft_label = self.base_dataset.soft_labels[real_idx]\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(soft_label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a083fb5f-550f-4cd6-b5f3-416b81b60f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders ready!\n"
     ]
    }
   ],
   "source": [
    "# Create datasets with transforms\n",
    "train_dataset = SoftTransformDataset(full_dataset, train_idx, train_transform)\n",
    "val_dataset = SoftTransformDataset(full_dataset, val_idx, eval_transform)\n",
    "test_dataset = SoftTransformDataset(full_dataset, test_idx, eval_transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"DataLoaders ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caed180-285d-43a0-9d3d-272efaf0abfa",
   "metadata": {},
   "source": [
    "#### MODEL SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7e3bff2-d662-4791-baaf-43365247eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Freeze all parameters first\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace fully connected layer with 6 outputs\n",
    "# --------------------------------------------------\n",
    "# Get input features of original fc layer\n",
    "in_features = model.fc.in_features\n",
    "\n",
    "# Replace with new fc layer (unfrozen by default)\n",
    "model.fc = nn.Linear(in_features, 6) # 6 emotion outputs (logits)\n",
    "\n",
    "# Move ENTIRE model to device AFTER architecture changes\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77ab5ca9-2b9c-49d1-8c0c-f5c57cb16ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de791035-b0ff-4039-9b3c-8e52faaabbd4",
   "metadata": {},
   "source": [
    "#### TRAINING FUNCTION TO CALCULATE OVERALL ACCURACY ON VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b03c10da-99d4-44a4-8022-0790a941e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_basic_soft(model, train_loader, val_loader, optimizer, device, epochs, checkpoint_dir=checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # KL Divergence loss for soft labels\n",
    "    criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, soft_labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            soft_labels = soft_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            log_probs = F.log_softmax(outputs, dim=1)\n",
    "            \n",
    "            loss = criterion(log_probs, soft_labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, soft_labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                hard_labels = torch.argmax(soft_labels, dim=1).to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                total += hard_labels.size(0)\n",
    "                correct += (predicted == hard_labels).sum().item()\n",
    "        \n",
    "        accuracy = 100.0 * correct / total\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {avg_train_loss:.4f} | Val Acc: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if accuracy > best_val_acc:\n",
    "            best_val_acc = accuracy\n",
    "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'best_basic_soft.pth'))\n",
    "    \n",
    "    print(f\"\\nBest validation accuracy: {best_val_acc:.2f}%\")\n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a954e-e377-4cac-9433-6c3382440531",
   "metadata": {},
   "source": [
    "#### RUN TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b81f563d-14c5-4a25-89f3-41afcac06a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] | Train Loss: 0.2820 | Val Acc: 56.23%\n",
      "Epoch [2/30] | Train Loss: 0.1829 | Val Acc: 57.24%\n",
      "Epoch [3/30] | Train Loss: 0.1554 | Val Acc: 57.24%\n",
      "Epoch [4/30] | Train Loss: 0.1443 | Val Acc: 59.26%\n",
      "Epoch [5/30] | Train Loss: 0.1296 | Val Acc: 58.92%\n",
      "Epoch [6/30] | Train Loss: 0.1208 | Val Acc: 59.93%\n",
      "Epoch [7/30] | Train Loss: 0.1083 | Val Acc: 58.59%\n",
      "Epoch [8/30] | Train Loss: 0.1013 | Val Acc: 57.91%\n",
      "Epoch [9/30] | Train Loss: 0.1049 | Val Acc: 58.25%\n",
      "Epoch [10/30] | Train Loss: 0.0930 | Val Acc: 60.27%\n",
      "Epoch [11/30] | Train Loss: 0.0870 | Val Acc: 60.61%\n",
      "Epoch [12/30] | Train Loss: 0.0836 | Val Acc: 58.59%\n",
      "Epoch [13/30] | Train Loss: 0.0832 | Val Acc: 60.27%\n",
      "Epoch [14/30] | Train Loss: 0.0809 | Val Acc: 59.26%\n",
      "Epoch [15/30] | Train Loss: 0.0738 | Val Acc: 60.27%\n",
      "Epoch [16/30] | Train Loss: 0.0659 | Val Acc: 59.60%\n",
      "Epoch [17/30] | Train Loss: 0.0658 | Val Acc: 59.26%\n",
      "Epoch [18/30] | Train Loss: 0.0683 | Val Acc: 59.93%\n",
      "Epoch [19/30] | Train Loss: 0.0655 | Val Acc: 58.92%\n",
      "Epoch [20/30] | Train Loss: 0.0703 | Val Acc: 60.27%\n",
      "Epoch [21/30] | Train Loss: 0.0701 | Val Acc: 60.27%\n",
      "Epoch [22/30] | Train Loss: 0.0618 | Val Acc: 58.92%\n",
      "Epoch [23/30] | Train Loss: 0.0629 | Val Acc: 59.93%\n",
      "Epoch [24/30] | Train Loss: 0.0722 | Val Acc: 60.27%\n",
      "Epoch [25/30] | Train Loss: 0.0577 | Val Acc: 58.25%\n",
      "Epoch [26/30] | Train Loss: 0.0582 | Val Acc: 57.58%\n",
      "Epoch [27/30] | Train Loss: 0.0600 | Val Acc: 58.59%\n",
      "Epoch [28/30] | Train Loss: 0.0505 | Val Acc: 56.23%\n",
      "Epoch [29/30] | Train Loss: 0.0544 | Val Acc: 59.60%\n",
      "Epoch [30/30] | Train Loss: 0.0572 | Val Acc: 59.26%\n",
      "\n",
      "Best validation accuracy: 60.61%\n",
      "Final best accuracy: 60.61%\n"
     ]
    }
   ],
   "source": [
    "best_acc = train_basic_soft(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    epochs=30\n",
    ")\n",
    "\n",
    "print(f\"Final best accuracy: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd67dd-a12f-405c-b5fe-223ccaea2903",
   "metadata": {},
   "source": [
    "#### LOAD THE SAVED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990b4f15-5fd7-44ce-8c8f-e12ce2dc2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate model architecture (same as training)\n",
    "model_s = models.resnet50(weights=None)  # Don't load pretrained\n",
    "\n",
    "# Replace FC layer (must match training architecture)\n",
    "in_features = model_s.fc.in_features\n",
    "model_s.fc = torch.nn.Linear(in_features, 6)  # Simple linear layer, 6 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a9a3c5-0e1e-4bd1-8ce6-a2bdb6873d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load saved best model weights\n",
    "checkpoint_path = checkpoint_path\n",
    "model_s.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model_s = model_s.to(device)\n",
    "model_s.eval()  # Set to evaluation mode\n",
    "\n",
    "# print(f\"Model loaded from {checkpoint_path}\")\n",
    "print(f\"Model is on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cfcb2e-63b7-48fc-885c-6eae4888b0cf",
   "metadata": {},
   "source": [
    "#### DEFINE EVALUATION METRICS (KL Divergence, Pearson correlation per class, Top-k accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97747e90-c02e-4997-8da0-48456826de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_divergence(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Computes average KL Divergence between ground truth soft labels and model predictions.\n",
    "    KL(P||Q) = sum(P * log(P/Q))\n",
    "    Lower is better (0 = perfect match).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_kl = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, soft_labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            soft_labels = soft_labels.to(device)  # [batch, 6] ground truth distributions\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = model(images)  # [batch, 6] logits\n",
    "            log_probs = F.log_softmax(outputs, dim=1)  # log(predicted probabilities)\n",
    "            \n",
    "            # KL Divergence: F.kl_div expects (log_q, p) where p is target\n",
    "            # reduction='batchmean' averages over batch\n",
    "            kl = F.kl_div(log_probs, soft_labels, reduction='batchmean')\n",
    "            \n",
    "            total_kl += kl.item() * images.size(0)\n",
    "            count += images.size(0)\n",
    "    \n",
    "    avg_kl = total_kl / count\n",
    "    return avg_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "193d48e7-e182-4fe8-bf1d-1062fe76764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pearson_correlation_per_class(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Computes Pearson correlation coefficient for each emotion class.\n",
    "    Returns dict with correlation for each emotion.\n",
    "    Range: -1 to 1, where 1 = perfect positive correlation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect all predictions and ground truths\n",
    "    all_true_soft = []   # List of [6] arrays\n",
    "    all_pred_soft = []   # List of [6] arrays\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, soft_labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)  # Predicted probabilities\n",
    "            \n",
    "            all_true_soft.extend(soft_labels.cpu().numpy())\n",
    "            all_pred_soft.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays [n_samples, 6]\n",
    "    all_true_soft = np.array(all_true_soft)\n",
    "    all_pred_soft = np.array(all_pred_soft)\n",
    "    \n",
    "    # Compute Pearson correlation for each emotion class\n",
    "    emotion_names = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
    "    correlations = {}\n",
    "    \n",
    "    for i, emotion in enumerate(emotion_names):\n",
    "        # Get true and predicted probabilities for this emotion\n",
    "        true_vals = all_true_soft[:, i]\n",
    "        pred_vals = all_pred_soft[:, i]\n",
    "        \n",
    "        # Compute Pearson r\n",
    "        r, p_value = pearsonr(true_vals, pred_vals)\n",
    "        correlations[emotion] = {\n",
    "            'correlation': r,\n",
    "            'p_value': p_value\n",
    "        }\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d35050f0-957e-4888-8d1c-0df287818a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_k_accuracy(model, val_loader, device, k=2):\n",
    "    \"\"\"\n",
    "    Computes Top-K accuracy: Is the true dominant emotion in the model's top K predictions?\n",
    "    For k=2: Is true label among the 2 highest predicted probabilities?\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, soft_labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            soft_labels = soft_labels.to(device)\n",
    "            \n",
    "            # Get hard labels (dominant emotion) from soft labels\n",
    "            true_labels = torch.argmax(soft_labels, dim=1)  # [batch]\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Get top-k predicted classes\n",
    "            # topk returns (values, indices)\n",
    "            _, top_k_indices = torch.topk(outputs, k, dim=1)  # [batch, k]\n",
    "            \n",
    "            # Check if true label is in top-k for each sample\n",
    "            # Expand true_labels to compare with top_k_indices\n",
    "            true_labels_expanded = true_labels.unsqueeze(1)  # [batch, 1]\n",
    "            matches = (top_k_indices == true_labels_expanded).any(dim=1)  # [batch]\n",
    "            \n",
    "            correct += matches.sum().item()\n",
    "            total += images.size(0)\n",
    "    \n",
    "    accuracy = 100.0 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce26f6-3663-4a0d-91c5-10f557244a7b",
   "metadata": {},
   "source": [
    "#### RUN EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74f4aa00-f05b-42ae-b045-798599890441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Computing KL Divergence on validation data...\n",
      "   Average KL Divergence: 0.1600\n",
      "   Interpretation: Good (small information loss)\n"
     ]
    }
   ],
   "source": [
    "# 1. KL Divergence\n",
    "print(\"\\n1. Computing KL Divergence on validation data...\")\n",
    "kl_div_score = compute_kl_divergence(model_s, val_loader, device)\n",
    "print(f\"   Average KL Divergence: {kl_div_score:.4f}\")\n",
    "print(f\"   Interpretation: \", end=\"\")\n",
    "if kl_div_score < 0.1:\n",
    "    print(\"Excellent (very close distributions)\")\n",
    "elif kl_div_score < 0.3:\n",
    "    print(\"Good (small information loss)\")\n",
    "elif kl_div_score < 0.6:\n",
    "    print(\"Moderate (some mismatch)\")\n",
    "else:\n",
    "    print(\"Poor (significant mismatch)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f62ef5e-b7e8-437a-b8c7-5b46b96f0bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Computing Pearson Correlation per Class on validation data...\n",
      "   Correlation (r) for each emotion:\n",
      "   -      anger:  0.419 (Moderate)\n",
      "   -    disgust:  0.759 (Excellent)\n",
      "   -       fear:  0.554 (Good)\n",
      "   -        joy:  0.715 (Excellent)\n",
      "   -    sadness:  0.711 (Excellent)\n",
      "   -   surprise:  0.528 (Good)\n"
     ]
    }
   ],
   "source": [
    "# 2. Pearson Correlation per Class\n",
    "print(\"\\n2. Computing Pearson Correlation per Class on validation data...\")\n",
    "correlations = compute_pearson_correlation_per_class(model_s, val_loader, device)\n",
    "print(\"   Correlation (r) for each emotion:\")\n",
    "for emotion, stats in correlations.items():\n",
    "    r = stats['correlation']\n",
    "    print(f\"   - {emotion:>10}: {r:>6.3f} \", end=\"\")\n",
    "    if r > 0.7:\n",
    "        print(\"(Excellent)\")\n",
    "    elif r > 0.5:\n",
    "        print(\"(Good)\")\n",
    "    elif r > 0.3:\n",
    "        print(\"(Moderate)\")\n",
    "    else:\n",
    "        print(\"(Weak)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faab9ae-2b26-4718-9379-8d8ac7f04216",
   "metadata": {},
   "source": [
    "| Emotion  | Correlation | What It Means                                  |\n",
    "| -------- | ----------- | ---------------------------------------------- |\n",
    "| Disgust  | 0.76        | Model excels at detecting disgust              |\n",
    "| Joy      | 0.72        | Strong performance on positive emotions        |\n",
    "| Sadness  | 0.71        | Good understanding of sadness                  |\n",
    "| Fear     | 0.55        | Moderate: Sometimes confused with disgust     |\n",
    "| Surprise | 0.53        | Moderate: Often confused with fear/joy        |\n",
    "| Anger    | 0.42        | Weakest: Model struggles with anger detection |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7d8aa62-3655-41a8-a54b-9abd693bb54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Average Correlation: 0.614\n"
     ]
    }
   ],
   "source": [
    "# Average correlation across all emotions\n",
    "avg_corr = np.mean([stats['correlation'] for stats in correlations.values()])\n",
    "print(f\"\\n   Average Correlation: {avg_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1968feb0-77bf-484e-9dfc-126d98a32cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Computing Top-2 Accuracy...\n",
      "   Top-2 Accuracy: 80.13%\n",
      "   (True dominant emotion is in model's top 2 predictions 80.13% of the time)\n"
     ]
    }
   ],
   "source": [
    "# 3. Top-2 Accuracy\n",
    "print(\"\\n3. Computing Top-2 Accuracy...\")\n",
    "top2_acc = compute_top_k_accuracy(model_s, val_loader, device, k=2)\n",
    "print(f\"   Top-2 Accuracy: {top2_acc:.2f}%\")\n",
    "print(f\"   (True dominant emotion is in model's top 2 predictions {top2_acc:.2f}% of the time)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "455c08fb-c6ef-480e-8574-f5dc79b5d708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Top-1 Accuracy: 60.61% (for reference)\n"
     ]
    }
   ],
   "source": [
    "# For comparison, also compute Top-1\n",
    "top1_acc = compute_top_k_accuracy(model_s, val_loader, device, k=1)\n",
    "print(f\"   Top-1 Accuracy: {top1_acc:.2f}% (for reference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a3f4b-dc1c-45a5-9178-4036bc97668d",
   "metadata": {},
   "source": [
    "## Emotion6 Image Classification Results\n",
    "\n",
    "### Model Architecture\n",
    "- **Base Model:** ResNet50 (pretrained on ImageNet)\n",
    "- **Modification:** Replaced final FC layer with 6 neurons (anger, disgust, fear, joy, sadness, surprise)\n",
    "- **Training:** Frozen convolutional layers, only FC layer trained\n",
    "- **Loss Function:** KL Divergence (soft labels) vs. Cross Entropy (hard labels)\n",
    "\n",
    "### Classification Accuracy\n",
    "| Metric | Result | Interpretation |\n",
    "|--------|--------|----------------|\n",
    "| **Top-1 Accuracy** | **60.61%** | Matches original CVPR paper baseline (64.72%) |\n",
    "| **Top-2 Accuracy** | **80.13%** | True emotion in model's top 2 predictions 80% of the time |\n",
    "\n",
    "### Soft Label Evaluation Metrics\n",
    "Unlike standard classification that uses hard labels (one-hot), Emotion6 provides emotion **probability distributions** from multiple annotators (e.g., 60% fear, 30% disgust, 10% surprise). Using KL Divergence loss during training teaches the model this nuance instead of forcing a single \"correct\" answer.\n",
    "\n",
    "| Metric | Result | Interpretation |\n",
    "|--------|--------|----------------|\n",
    "| **KL Divergence** | **0.16** | Low information loss between predicted and true distributions |\n",
    "| **Avg Pearson Correlation** | **0.61** | Strong linear relationship between predicted and true emotion probabilities |\n",
    "\n",
    "### Per-Emotion Pearson Correlation\n",
    "| Emotion | Correlation | Performance |\n",
    "|---------|-------------|-------------|\n",
    "| Disgust | 0.76 | Excellent |\n",
    "| Joy | 0.72 | Excellent |\n",
    "| Sadness | 0.71 | Excellent |\n",
    "| Fear | 0.55 | Good |\n",
    "| Surprise | 0.53 | Good |\n",
    "| Anger | 0.42 | Moderate (improvement needed) |\n",
    "\n",
    "### Key Takeaways\n",
    "- **Soft-label training** captures emotion ambiguity better than hard labels, yielding more robust probability distributions\n",
    "- **80.13% top-2 accuracy** demonstrates practical utility for real-world applications where narrowing to 2 emotions is valuable\n",
    "- **Low KL divergence (0.16)** confirms the model learns nuanced distributions rather than overconfident single predictions\n",
    "- **Per-class correlation analysis** identifies anger detection as the primary area for architectural improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ef76e-58b6-41ef-bf41-a51c6fe1827a",
   "metadata": {},
   "source": [
    "#### RUN EVALUATION ON TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "214ea08f-0e87-4bde-a795-7c8669f19f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Computing KL Divergence on test data...\n",
      "   Average KL Divergence: 0.1836\n",
      "   Interpretation: Good (small information loss)\n"
     ]
    }
   ],
   "source": [
    "# 1. KL Divergence\n",
    "print(\"\\n1. Computing KL Divergence on test data...\")\n",
    "kl_div_score = compute_kl_divergence(model_s, test_loader, device)\n",
    "print(f\"   Average KL Divergence: {kl_div_score:.4f}\")\n",
    "print(f\"   Interpretation: \", end=\"\")\n",
    "if kl_div_score < 0.1:\n",
    "    print(\"Excellent (very close distributions)\")\n",
    "elif kl_div_score < 0.3:\n",
    "    print(\"Good (small information loss)\")\n",
    "elif kl_div_score < 0.6:\n",
    "    print(\"Moderate (some mismatch)\")\n",
    "else:\n",
    "    print(\"Poor (significant mismatch)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "344bd3c1-c5ec-409b-8f69-6c5d268ea53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Computing Pearson Correlation per Class on test data...\n",
      "   Correlation (r) for each emotion:\n",
      "   -      anger:  0.378 (Moderate)\n",
      "   -    disgust:  0.722 (Excellent)\n",
      "   -       fear:  0.564 (Good)\n",
      "   -        joy:  0.681 (Good)\n",
      "   -    sadness:  0.582 (Good)\n",
      "   -   surprise:  0.534 (Good)\n"
     ]
    }
   ],
   "source": [
    "# 2. Pearson Correlation per Class\n",
    "print(\"\\n2. Computing Pearson Correlation per Class on test data...\")\n",
    "correlations = compute_pearson_correlation_per_class(model_s, test_loader, device)\n",
    "print(\"   Correlation (r) for each emotion:\")\n",
    "for emotion, stats in correlations.items():\n",
    "    r = stats['correlation']\n",
    "    print(f\"   - {emotion:>10}: {r:>6.3f} \", end=\"\")\n",
    "    if r > 0.7:\n",
    "        print(\"(Excellent)\")\n",
    "    elif r > 0.5:\n",
    "        print(\"(Good)\")\n",
    "    elif r > 0.3:\n",
    "        print(\"(Moderate)\")\n",
    "    else:\n",
    "        print(\"(Weak)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823964b-a9b9-4e7f-ab0a-c6578af4cbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a8733-c015-4299-a2b8-b02d7a2da977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (CB)",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
